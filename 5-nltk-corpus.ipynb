{
 "metadata": {
  "name": "",
  "signature": "sha256:9a74cb89b53d0eb17b9316f7e412d7de883cb7d1ad26230f7d2b86d257757ff7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Resumen NLTK: Acceso a corpus de texto y recursos l\u00e9xicos\n",
      "\n",
      "Este resumen se corresponde con el cap\u00edtulo 2 del NLTK Book [Accessing Text Corpora and Lexical Resources](http://www.nltk.org/book_1ed/ch02.html). La lectura del cap\u00edtulo es muy recomendable.\n",
      "\n",
      "\n",
      "## Corpus no anotados: el Proyecto Gutenberg\n",
      "\n",
      "NLTK no da acceso directo a varias colecciones de textos. Para empezar, vamos a juguetear un poco con los libros del [Proyecto Gutenberg](http://www.gutenberg.org), un repositorio p\u00fablico de libros libres y/o sin derechos de copyright en vigor.\n",
      "Antes de nada, necesitamos importar el m\u00f3dulo `gutenberg` que est\u00e1 en la librer\u00eda `nltk.corpus`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from nltk.corpus import gutenberg\n",
      "from __future__ import division"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Podemos listar el cat\u00e1alogo de libros del Proyecto Gutenberg disponibles desde NLTK a trav\u00e9s del m\u00e9todo `nltk.corpus.gutenberg.fileids`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print gutenberg.fileids()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Para cargar alguno de estos libros en variables y poder manipularlos directamente, podemos utilizar varios m\u00e9todos.\n",
      "\n",
      "- `gutenberg.raw` recupera el texto como una \u00fanica cadena de caracteres.\n",
      "- `gutenberg.words` recupera el texto tokenizado en palabras. El m\u00e9todo devuelve una lista palabras.\n",
      "- `gutenberg.sents` recupera el texto segmentado por oraciones. El m\u00e9todo devuelve una lista de oraciones. Cada oraci\u00f3n es a su vez una lista de palabras.\n",
      "- `gutenberg.paras` recupera el texto  segmentado por p\u00e1rrafos. El m\u00e9todo devuelve una lista de p\u00e1rrafos.  Cada p\u00e1rrafo es una lista de oraciones, cada oraci\u00f3n es a su vez una lista de palabras."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# cargo la vesi\u00f3n 'cruda' de un par de libros. Como son libros del Proyecto Gutenberg, se trata de ficheros en texto plano\n",
      "alice = gutenberg.raw(\"carroll-alice.txt\")\n",
      "print alice[:200] # imprimo los primeros 200 caracteres del libro de Alicia\n",
      "\n",
      "bible = gutenberg.raw(\"bible-kjv.txt\")\n",
      "print bible[:200] # imprimo los primeros 200 caracteres de la Biblia"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
        "\n",
        "CHAPTER I. Down the Rabbit-Hole\n",
        "\n",
        "Alice was beginning to get very tired of sitting by her sister on the\n",
        "bank, and of having nothing to do: once\n",
        "[The King James Bible]\n",
        "\n",
        "The Old Testament of the King James Bible\n",
        "\n",
        "The First Book of Moses:  Called Genesis\n",
        "\n",
        "\n",
        "1:1 In the beginning God created the heaven and the earth.\n",
        "\n",
        "1:2 And the earth was without \n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# segmentamos el texto en palabras teniendo en cuenta los espacios \n",
      "bible_tokens = bible.split()\n",
      "# cargamos la versi\u00f3n de la Biblia segmentado en palabras\n",
      "bible_words = gutenberg.words(\"bible-kjv.txt\")\n",
      "\n",
      "# no da el mismo n\u00famero de tokens\n",
      "print len(bible_tokens), len(bible_words)\n",
      "\n",
      "print bible_tokens[:100]\n",
      "print \"-----------------------------------------------\"\n",
      "print bible_words[:100]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "821133 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1010654\n",
        "['[The', 'King', 'James', 'Bible]', 'The', 'Old', 'Testament', 'of', 'the', 'King', 'James', 'Bible', 'The', 'First', 'Book', 'of', 'Moses:', 'Called', 'Genesis', '1:1', 'In', 'the', 'beginning', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth.', '1:2', 'And', 'the', 'earth', 'was', 'without', 'form,', 'and', 'void;', 'and', 'darkness', 'was', 'upon', 'the', 'face', 'of', 'the', 'deep.', 'And', 'the', 'Spirit', 'of', 'God', 'moved', 'upon', 'the', 'face', 'of', 'the', 'waters.', '1:3', 'And', 'God', 'said,', 'Let', 'there', 'be', 'light:', 'and', 'there', 'was', 'light.', '1:4', 'And', 'God', 'saw', 'the', 'light,', 'that', 'it', 'was', 'good:', 'and', 'God', 'divided', 'the', 'light', 'from', 'the', 'darkness.', '1:5', 'And', 'God', 'called', 'the', 'light', 'Day,', 'and', 'the', 'darkness']\n",
        "-----------------------------------------------\n",
        "['[', 'The', 'King', 'James', 'Bible', ']', 'The', 'Old', 'Testament', 'of', 'the', 'King', 'James', 'Bible', 'The', 'First', 'Book', 'of', 'Moses', ':', 'Called', 'Genesis', '1', ':', '1', 'In', 'the', 'beginning', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth', '.', '1', ':', '2', 'And', 'the', 'earth', 'was', 'without', 'form', ',', 'and', 'void', ';', 'and', 'darkness', 'was', 'upon', 'the', 'face', 'of', 'the', 'deep', '.', 'And', 'the', 'Spirit', 'of', 'God', 'moved', 'upon', 'the', 'face', 'of', 'the', 'waters', '.', '1', ':', '3', 'And', 'God', 'said', ',', 'Let', 'there', 'be', 'light', ':', 'and', 'there', 'was', 'light', '.', '1', ':', '4', 'And', 'God', 'saw', 'the', 'light', ',', 'that', 'it']\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# cargo la versi\u00f3n de Alicia segmentada en palabras\n",
      "alice_words = gutenberg.words(\"carroll-alice.txt\")\n",
      "print alice_words[:20] # imprimo las primeros 20 palabras\n",
      "print len(alice_words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['[', 'Alice', \"'\", 's', 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', '1865', ']', 'CHAPTER', 'I', '.', 'Down', 'the', 'Rabbit', '-', 'Hole']\n",
        "34110\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# cargo la versi\u00f3n de Alicia segmentada en oraciones\n",
      "alice_sents = gutenberg.sents(\"carroll-alice.txt\")\n",
      "print \"Alice tiene\", len(alice_sents), \"oraciones\"\n",
      "print alice_sents[2:5] # imprimo la tercera, cuarta y quinta oraci\u00f3n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Alice tiene "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2026 oraciones\n",
        "[['Down', 'the', 'Rabbit', '-', 'Hole'], ['Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', ',', 'and', 'of', 'having', 'nothing', 'to', 'do', ':', 'once', 'or', 'twice', 'she', 'had', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', ',', 'but', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', ',', \"'\", 'and', 'what', 'is', 'the', 'use', 'of', 'a', 'book', \",'\", 'thought', 'Alice', \"'\", 'without', 'pictures', 'or', 'conversation', '?'], [\"'\"]]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# cargo la versi\u00f3n de Alicia segmentada en p\u00e1rrafos\n",
      "alice_paras = gutenberg.paras(\"carroll-alice.txt\")\n",
      "print \"Alice tiene\", len(alice_paras), \"p\u00e1rrafos\"\n",
      "\n",
      "# imprimo los cinco primeros\n",
      "for para in alice_paras[:5]:\n",
      "    print para\n",
      "    print \"-------------------\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Alice tiene "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "817 p\u00e1rrafos\n",
        "[['[', 'Alice', \"'\", 's', 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', '1865', ']']]\n",
        "-------------------\n",
        "[['CHAPTER', 'I', '.'], ['Down', 'the', 'Rabbit', '-', 'Hole']]\n",
        "-------------------\n",
        "[['Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', ',', 'and', 'of', 'having', 'nothing', 'to', 'do', ':', 'once', 'or', 'twice', 'she', 'had', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', ',', 'but', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', ',', \"'\", 'and', 'what', 'is', 'the', 'use', 'of', 'a', 'book', \",'\", 'thought', 'Alice', \"'\", 'without', 'pictures', 'or', 'conversation', '?'], [\"'\"]]\n",
        "-------------------\n",
        "[['So', 'she', 'was', 'considering', 'in', 'her', 'own', 'mind', '(', 'as', 'well', 'as', 'she', 'could', ',', 'for', 'the', 'hot', 'day', 'made', 'her', 'feel', 'very', 'sleepy', 'and', 'stupid', '),', 'whether', 'the', 'pleasure', 'of', 'making', 'a', 'daisy', '-', 'chain', 'would', 'be', 'worth', 'the', 'trouble', 'of', 'getting', 'up', 'and', 'picking', 'the', 'daisies', ',', 'when', 'suddenly', 'a', 'White', 'Rabbit', 'with', 'pink', 'eyes', 'ran', 'close', 'by', 'her', '.']]\n",
        "-------------------\n",
        "[['There', 'was', 'nothing', 'so', 'VERY', 'remarkable', 'in', 'that', ';', 'nor', 'did', 'Alice', 'think', 'it', 'so', 'VERY', 'much', 'out', 'of', 'the', 'way', 'to', 'hear', 'the', 'Rabbit', 'say', 'to', 'itself', ',', \"'\", 'Oh', 'dear', '!'], ['Oh', 'dear', '!'], ['I', 'shall', 'be', 'late', '!'], [\"'\", '(', 'when', 'she', 'thought', 'it', 'over', 'afterwards', ',', 'it', 'occurred', 'to', 'her', 'that', 'she', 'ought', 'to', 'have', 'wondered', 'at', 'this', ',', 'but', 'at', 'the', 'time', 'it', 'all', 'seemed', 'quite', 'natural', ');', 'but', 'when', 'the', 'Rabbit', 'actually', 'TOOK', 'A', 'WATCH', 'OUT', 'OF', 'ITS', 'WAISTCOAT', '-', 'POCKET', ',', 'and', 'looked', 'at', 'it', ',', 'and', 'then', 'hurried', 'on', ',', 'Alice', 'started', 'to', 'her', 'feet', ',', 'for', 'it', 'flashed', 'across', 'her', 'mind', 'that', 'she', 'had', 'never', 'before', 'seen', 'a', 'rabbit', 'with', 'either', 'a', 'waistcoat', '-', 'pocket', ',', 'or', 'a', 'watch', 'to', 'take', 'out', 'of', 'it', ',', 'and', 'burning', 'with', 'curiosity', ',', 'she', 'ran', 'across', 'the', 'field', 'after', 'it', ',', 'and', 'fortunately', 'was', 'just', 'in', 'time', 'to', 'see', 'it', 'pop', 'down', 'a', 'large', 'rabbit', '-', 'hole', 'under', 'the', 'hedge', '.']]\n",
        "-------------------\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "F\u00edjate en que cada m\u00e9todo devuelve una estructura de datos diferente: desde una \u00fanica cadena a listas de listas anidadas. Para que tengas claro las dimensiones de cada uno, podemos imprimir el n\u00famero de caracteres, palabras, oraciones y p\u00e1rrafos del libro. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(alice), \"caracteres\" \n",
      "print len(alice_words), \"palabras\"\n",
      "print len(alice_sents), \"oraciones\"\n",
      "print len(alice_paras), \"p\u00e1rrafos\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "144395 caracteres\n",
        "34110 palabras\n",
        "2026 oraciones\n",
        "817 p\u00e1rrafos\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Vamos a imprimir algunas estad\u00edsticas para todos los libros del Proyecto Gutenberg disponibles. Para cada libro, impriremos por pantalla el promedio de caracteres por palabra, el promedio de palabras por oraci\u00f3n y el promedio de oraciones por p\u00e1rrafo."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# para cada libro que est\u00e1 disponible en el objeto gutenberg\n",
      "for libro in gutenberg.fileids():\n",
      "    caracteres = len(gutenberg.raw(libro))\n",
      "    palabras = len(gutenberg.words(libro))\n",
      "    oraciones = len(gutenberg.sents(libro))\n",
      "    parrafos = len(gutenberg.paras(libro))\n",
      "    print libro[:-4], \"\\t\", round(caracteres/palabras, 2), \"\\t\", round(palabras/oraciones, 2), \"\\t\", round(oraciones/parrafos, 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "austen-emma \t4.61 \t21.12 \t3.84\n",
        "austen-persuasion"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \t4.75 \t23.53 \t4.04\n",
        "austen-sense"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \t4.75 \t23.89 \t3.18\n",
        "bible-kjv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \t4.29 \t33.57 \t1.22\n",
        "blake-poems"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \t4.57 \t18.28 \t1.61\n",
        "bryant-stories"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \t4.49 \t17.27 \t2.7\n",
        "burgess-busterbrown"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \t4.46 \t17.07 \t4.18\n",
        "carroll-alice"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \t4.23 \t16.84 \t2.48\n",
        "chesterton-ball"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \t4.72 \t17.75 \t3.4\n",
        "chesterton-brown"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \t4.72 \t19.67 \t3.77\n",
        "chesterton-thursday"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \t4.63 \t16.12 \t3.33\n",
        "edgeworth-parents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \t4.44 \t17.62 \t3.21\n",
        "melville-moby_dick"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \t4.77 \t24.16 \t3.87\n",
        "milton-paradise"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \t4.84 \t52.31 \t63.83\n",
        "shakespeare-caesar"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \t4.35 \t11.94 \t2.91\n",
        "shakespeare-hamlet"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \t4.36 \t12.03 \t3.27\n",
        "shakespeare-macbeth"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \t4.34 \t12.13 \t2.81\n",
        "whitman-leaves"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \t4.59 \t35.26 \t1.77\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "El m\u00f3dulo `nltk.corpus` permite acceder a otras colecciones de textos en otras lenguas ([lista completa aqu\u00ed](http://www.nltk.org/book_1ed/ch02.html#tab-corpora)). Vamos a probar con un corpus de noticias en castellano llamado `cess_esp` que incluye anotaci\u00f3n morfo-sint\u00e1ctica."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import cess_esp\n",
      "\n",
      "# la versi\u00f3n en crudo de este corpus contiene informaci\u00f3n morfosint\u00e1ctica con un formato que todav\u00eda no conocemos.\n",
      "# en este caso, pasamos directamente a trabajar con los textos segmentados\n",
      "\n",
      "# cargo el primer documento del corpus segmentado en palabras\n",
      "palabras = cess_esp.words(cess_esp.fileids()[0])\n",
      "print palabras[:50]\n",
      "\n",
      "print \"----------------------\"\n",
      "\n",
      "# y segmentado en oraciones\n",
      "oraciones = cess_esp.sents(cess_esp.fileids()[0])\n",
      "print oraciones[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['El', 'grupo', 'estatal', 'Electricit\\xe9_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunci\\xf3', 'hoy', ',', 'jueves', ',', 'la', 'compra', 'del', '51_por_ciento', 'de', 'la', 'empresa', 'mexicana', 'Electricidad_\\xc1guila_de_Altamira', '-Fpa-', 'EAA', '-Fpt-', ',', 'creada', 'por', 'el', 'japon\\xe9s', 'Mitsubishi_Corporation', 'para', 'poner_en_marcha', 'una', 'central', 'de', 'gas', 'de', '495', 'megavatios', '.', 'Una', 'portavoz', 'de', 'EDF', 'explic\\xf3', 'a', 'EFE', 'que', 'el', 'proyecto']\n",
        "----------------------\n",
        "[['El', 'grupo', 'estatal', 'Electricit\\xe9_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunci\\xf3', 'hoy', ',', 'jueves', ',', 'la', 'compra', 'del', '51_por_ciento', 'de', 'la', 'empresa', 'mexicana', 'Electricidad_\\xc1guila_de_Altamira', '-Fpa-', 'EAA', '-Fpt-', ',', 'creada', 'por', 'el', 'japon\\xe9s', 'Mitsubishi_Corporation', 'para', 'poner_en_marcha', 'una', 'central', 'de', 'gas', 'de', '495', 'megavatios', '.'], ['Una', 'portavoz', 'de', 'EDF', 'explic\\xf3', 'a', 'EFE', 'que', 'el', 'proyecto', 'para', 'la', 'construcci\\xf3n', 'de', 'Altamira_2', ',', 'al', 'norte', 'de', 'Tampico', ',', 'prev\\xe9', 'la', 'utilizaci\\xf3n', 'de', 'gas', 'natural', 'como', 'combustible', 'principal', 'en', 'una', 'central', 'de', 'ciclo', 'combinado', 'que', 'debe', 'empezar', 'a', 'funcionar', 'en', 'mayo_del_2002', '.'], ['La', 'electricidad', 'producida', 'pasar\\xe1', 'a', 'la', 'red', 'el\\xe9ctrica', 'p\\xfablica', 'de', 'M\\xe9xico', 'en_virtud_de', 'un', 'acuerdo', 'de', 'venta', 'de', 'energ\\xeda', 'de', 'EAA', 'con', 'la', 'Comisi\\xf3n_Federal_de_Electricidad', '-Fpa-', 'CFE', '-Fpt-', 'por', 'una', 'duraci\\xf3n', 'de', '25', 'a\\xf1os', '.'], ['EDF', ',', 'que', 'no', 'quiso', 'revelar', 'cu\\xe1nto', '*0*', 'pag\\xf3', 'por', 'su', 'participaci\\xf3n', 'mayoritaria', 'en', 'EAA', ',', 'intervendr\\xe1', 'como', 'asistente', 'en', 'la', 'construcci\\xf3n', 'de', 'Altamira_2', 'y', ',', 'posteriormente', ',', '*0*', 'se', 'encargar\\xe1', 'de', 'explotarla', 'como', 'principal', 'accionista', '.'], ['EDF', 'y', 'Mitsubishi', 'participaron', 'en', '1998', 'en', 'la', 'licitaci\\xf3n', 'de', 'licencias', 'para', 'construir', 'centrales', 'el\\xe9ctricas', 'en', 'M\\xe9xico', 'y', '*0*', 'se', 'quedaron', 'con', 'dos', 'cada', 'una', ':', 'R\\xedo_Bravo', 'y', 'Saltillo', 'para', 'la', 'compa\\xf1\\xeda', 'francesa', 'y', 'Altamira', 'y', 'Tuxp\\xe1n', 'para', 'la', 'japonesa', '.']]\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "De manera similar a como hemos hecho sacando estad\u00edsticias de las obras disponibles en el corpus `gutenberg`, vamos a calcular la longitud promedio de palabras y el n\u00famero de palabras promedio por oraci\u00f3n, para los diez primeros documentos de este corpus `cess_esp`. \n",
      "\n",
      "F\u00edjate en la estructura de este ejemplo: contiene bucles anidados."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# para cada documento que est\u00e1 entre los 10 primeros del corpus\n",
      "for documento in cess_esp.fileids()[:10]:    \n",
      "    # carga el texto segmentado en palabras\n",
      "    palabras = cess_esp.words(documento)\n",
      "    # y en oraciones\n",
      "    oraciones = cess_esp.sents(documento)\n",
      "    \n",
      "    # pon el contador de caracteres a 0\n",
      "    caracteres = 0    \n",
      "    # para cada palabra dentro de la lista de palabras del documento\n",
      "    for palabra in palabras:\n",
      "        # ve sumando al contador el n\u00famero de caracteres que tiene la palabra en cuesti\u00f3n\n",
      "        caracteres = caracteres + len(palabra)\n",
      "    \n",
      "    # cuando hayas terminado, divide la longitud total del texto entre el n\u00famero de palabras\n",
      "    longitud_promedio = caracteres / len(palabras)\n",
      "    \n",
      "    # imprime el nombre del documento, la longitud de la palabra y el n\u00famero de palabras por oraci\u00f3n\n",
      "    print documento[:-4], \"\\t\", round(longitud_promedio, 2), \"\\t\", round(len(palabras)/len(oraciones), 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10017_20000413 \t5.04 \t42.17\n",
        "10044_20000313 \t4.27 \t47.63\n",
        "10049_20001114 \t5.84 \t31.11\n",
        "10055_20000713 \t4.98 \t30.89\n",
        "10080_20000914 \t4.6 \t34.0\n",
        "10084_20000313_1 \t4.54 \t42.36"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10084_20000313_2 \t4.71 \t24.0\n",
        "10127_20001013_1 \t4.63 \t40.1\n",
        "10127_20001013_2 \t4.71 \t32.1\n",
        "10127_20001013_3 \t4.72 \t35.29\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Los libros del Proyecto Gutenberg constituyen el tipo de corpus m\u00e1s sencillo: no est\u00e1 anotado (no incluye ning\u00fan tipo de informaci\u00f3n ling\u00fc\u00edstica) ni categorizado. \n",
      "\n",
      "## Corpus categorizados y anotados: el Corpus de Brown\n",
      "\n",
      "El Corpus de Brown fue el primer gran corpus orientado a tareas de PLN. Desarrollado en la Universidad de Brown, contiene m\u00e1s de un mill\u00f3n de palabras provenientes de 500 fuentes.  La principal catacter\u00edstica de este corpus es que sus textos est\u00e1n categorizados por g\u00e9nero. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from nltk.corpus import brown"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Como en los libros del Proyecto Gutenberg, aqu\u00ed tambi\u00e9n podemos imprimir los nombres de los ficheros. En este caso son poco significativos, nos nos dicen nada del contenido."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Brown est\u00e1 formado por 500 documentos\n",
      "print len(brown.fileids())\n",
      "# imprimimos solos los 10 primeros\n",
      "print brown.fileids()[:10] "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "500\n",
        "['ca01', 'ca02', 'ca03', 'ca04', 'ca05', 'ca06', 'ca07', 'ca08', 'ca09', 'ca10']\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### El corpus de Brown est\u00e1 categorizado por g\u00e9neros\n",
      "\n",
      "Una de las principales diferencias con otros corpus vistos anteriormente es que el de Brown est\u00e1 categorizado: los textos est\u00e1n agrupados seg\u00fan su g\u00e9nero o tem\u00e1tica. Y en este caso, los nombres de las categor\u00edas s\u00ed nos permiten intuir el contenido de los textos."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print brown.categories()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "De manera similar a los libros del Proyecto Gutenberg, podemos acceder a los textos de este corpus a trav\u00e9s de los m\u00e9todos `brown.raw`, `brown.words`, `brown.sents` y `brown.paras`. Adem\u00e1s, podemos acceder a una categor\u00eda de textos concretas si lo especificamos como argumento."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "news_words = brown.words(categories=\"news\")\n",
      "scifi_sents = brown.sents(categories=\"science_fiction\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print news_words[:50]\n",
      "print \"-----------------------\"\n",
      "print scifi_sents[:3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.', 'The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise']\n",
        "-----------------------\n",
        "[['Now', 'that', 'he', 'knew', 'himself', 'to', 'be', 'self', 'he', 'was', 'free', 'to', 'grok', 'ever', 'closer', 'to', 'his', 'brothers', ',', 'merge', 'without', 'let', '.'], [\"Self's\", 'integrity', 'was', 'and', 'is', 'and', 'ever', 'had', 'been', '.'], ['Mike', 'stopped', 'to', 'cherish', 'all', 'his', 'brother', 'selves', ',', 'the', 'many', 'threes-fulfilled', 'on', 'Mars', ',', 'corporate', 'and', 'discorporate', ',', 'the', 'precious', 'few', 'on', 'Earth', '--', 'the', 'unknown', 'powers', 'of', 'three', 'on', 'Earth', 'that', 'would', 'be', 'his', 'to', 'merge', 'with', 'and', 'cherish', 'now', 'that', 'at', 'last', 'long', 'waiting', 'he', 'grokked', 'and', 'cherished', 'himself', '.']]\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Vamos a sacar provecho de la categorizaci\u00f3n de los textos de este corpus. Para ello, vamos a calcular la frecuencia de distribuci\u00f3n de distintos verbos modales para cada categor\u00eda. Para ello vamos a calcular una distribuci\u00f3n de frecuencia condicional que calcule la frecuencia de cada palabra para cada categor\u00eda.\n",
      "\n",
      "No te preocupes si no entiendes la sintaxis para crear tablas de frecuencias condicionales a trav\u00e9s del objeto `ConditionalFreqDist`. Cr\u00e9eme, ese objeto calcula frecuencias de palabras atendiendo a la categor\u00eda en la que aparecen y crea una especie de diccionario de diccionarios."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from nltk import ConditionalFreqDist\n",
      "modals = \"can could would should must may might\".split()\n",
      "modals_cfd = ConditionalFreqDist(\n",
      "                          (category, word) \n",
      "                          for category in brown.categories() \n",
      "                          for word in brown.words(categories=category)\n",
      "                        )\n",
      "\n",
      "# la sintaxis anterior de anidar bucles for es un poco compleja y no la hemos visto\n",
      "# sin necesidad de profundizar m\u00e1s, las \u00faltimas tres l\u00edneas son equivalentes a:\n",
      "#for category in brown.categories():\n",
      "#    for word in brown.words(categories=category):\n",
      "#        ConditionalFreqDist(category, word)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Una vez tenemos calculada la frecuencia de distribuci\u00f3n condicional, podemos pintar los valores f\u00e1cilmente en forma de tabla a trav\u00e9s del m\u00e9todo `.tabulate`, especificando como condiciones cada una de las categor\u00edas, y como muestras los verbos modales del ingl\u00e9s que hemos definido."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "modals_cfd.tabulate(conditions=brown.categories(), samples=modals)\n",
      "\n",
      "# imprimo solo algunos verbos modales para la categor\u00eda fiction\n",
      "modals_cfd.tabulate(conditions=[\"fiction\"], samples=[\"can\", \"should\", \"would\"])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                 can could would should must  may might\n",
        "      adventure   46  151  191   15   27    5   58\n",
        " belles_lettres  246  213  392  102  170  207  113\n",
        "      editorial  121   56  180   88   53   74   39\n",
        "        fiction   37  166  287   35   55    8   44\n",
        "     government  117   38  120  112  102  153   13\n",
        "        hobbies  268   58   78   73   83  131   22\n",
        "          humor   16   30   56    7    9    8    8\n",
        "        learned  365  159  319  171  202  324  128\n",
        "           lore  170  141  186   76   96  165   49\n",
        "        mystery   42  141  186   29   30   13   57\n",
        "           news   93   86  244   59   50   66   38\n",
        "       religion   82   59   68   45   54   78   12\n",
        "        reviews   45   40   47   18   19   45   26\n",
        "        romance   74  193  244   32   45   11   51\n",
        "science_fiction   16   49   79    3    8    4   12\n",
        "         can should would\n",
        "fiction   37   35  287\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Las cifras que hemos mostrado en las tablas anteriores se refieren a las frecuencias absolutas de cada verbo modal en cada categor\u00eda. Realizar comparaciones as\u00ed no es acertado, porque es posible que cada categor\u00eda tenga un n\u00famero de documentos (y de palabras) diferente. \n",
      "\n",
      "Vamos a comprobar si esto es cierto. \u00bfEst\u00e1 equilibrada la colecci\u00f3n o tenemos algunos g\u00e9neros sobrerrepresentados?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for categoria in brown.categories():\n",
      "    print categoria, len(brown.words(categories=categoria))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "adventure "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "69342\n",
        "belles_lettres "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "173096\n",
        "editorial "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "61604\n",
        "fiction "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "68488\n",
        "government "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "70117\n",
        "hobbies "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "82345\n",
        "humor "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "21695\n",
        "learned "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "181888\n",
        "lore "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "110299\n",
        "mystery "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "57169\n",
        "news "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100554\n",
        "religion "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "39399\n",
        "reviews "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "40704\n",
        "romance "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "70022\n",
        "science_fiction 14470\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Como vemos, el n\u00famero de palabras no est\u00e1 equilibado. Tenemos muchos m\u00e1s datos en las categor\u00edas `belles_lettres` y `learned` que en `science_fiction` o `humor`, por ejemplo.\n",
      "\n",
      "Calculemos a continuaci\u00f3n la frecuencia relativa de estos verbos modales, atendiendo al g\u00e9nero. Para ello, necesitamos dividir la frecuencia absoluta de cada modal entre el n\u00famero de palabras total de cada categor\u00eda."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for categoria in brown.categories():\n",
      "    # \u00bfcu\u00e1ntas palabras tenemos en cada categor\u00eda?\n",
      "    longitud = len(brown.words(categories=categoria))\n",
      "    print \"\\n\", categoria\n",
      "    print\"----------------------\"\n",
      "    for palabra in modals:\n",
      "        print palabra, \"->\", modals_cfd[categoria][palabra]/longitud"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "adventure\n",
        "----------------------\n",
        "can -> 0.000663378616135\n",
        "could -> 0.00217761241383\n",
        "would -> 0.00275446338438\n",
        "should -> 0.000216319113957\n",
        "must -> 0.000389374405122\n",
        "may -> 7.2106371319e-05\n",
        "might -> 0.0008364339073\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "belles_lettres\n",
        "----------------------\n",
        "can -> 0.00142117668808\n",
        "could -> 0.0012305310348\n",
        "would -> 0.00226463927532\n",
        "should -> 0.000589268382863\n",
        "must -> 0.000982113971438\n",
        "may -> 0.00119586818875\n",
        "might -> 0.000652816933956\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "editorial\n",
        "----------------------\n",
        "can -> 0.00196415817155\n",
        "could -> 0.000909031881047\n",
        "would -> 0.00292188818908\n",
        "should -> 0.00142847867022\n",
        "must -> 0.000860333744562\n",
        "may -> 0.00120122069995\n",
        "might -> 0.0006330757743\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "fiction\n",
        "----------------------\n",
        "can -> 0.000540240626095\n",
        "could -> 0.00242378226843\n",
        "would -> 0.00419051512674\n",
        "should -> 0.00051103843009\n",
        "must -> 0.000803060390141\n",
        "may -> 0.000116808784021\n",
        "might -> 0.000642448312113\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "government\n",
        "----------------------\n",
        "can -> 0.00166863955959\n",
        "could -> 0.000541951309953\n",
        "would -> 0.00171142518933\n",
        "should -> 0.0015973301767\n",
        "must -> 0.00145471141093\n",
        "may -> 0.00218206711639\n",
        "might -> 0.00018540439551\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hobbies\n",
        "----------------------\n",
        "can -> 0.00325459955067\n",
        "could -> 0.0007043536341\n",
        "would -> 0.000947234197583\n",
        "should -> 0.000886514056713\n",
        "must -> 0.00100795433845\n",
        "may -> 0.00159086769081\n",
        "might -> 0.000267168619831\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "humor\n",
        "----------------------\n",
        "can -> 0.000737497119152\n",
        "could -> 0.00138280709841\n",
        "would -> 0.00258123991703\n",
        "should -> 0.000322654989629\n",
        "must -> 0.000414842129523\n",
        "may -> 0.000368748559576\n",
        "might -> 0.000368748559576\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "learned\n",
        "----------------------\n",
        "can -> 0.0020067294159\n",
        "could -> 0.000874164320901\n",
        "would -> 0.00175382653061\n",
        "should -> 0.000940138986629\n",
        "must -> 0.00111057353976\n",
        "may -> 0.00178131597467\n",
        "might -> 0.000703729767769\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "lore\n",
        "----------------------\n",
        "can -> 0.00154126510666\n",
        "could -> 0.001278343412\n",
        "would -> 0.001686325352\n",
        "should -> 0.000689036165332\n",
        "must -> 0.000870361471999\n",
        "may -> 0.00149593378\n",
        "might -> 0.000444247001333\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "mystery\n",
        "----------------------\n",
        "can -> 0.00073466389127\n",
        "could -> 0.00246637163498\n",
        "would -> 0.00325351151848\n",
        "should -> 0.000507267924924\n",
        "must -> 0.000524759922336\n",
        "may -> 0.000227395966345\n",
        "might -> 0.000997043852438\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "news\n",
        "----------------------\n",
        "can -> 0.00092487618593\n",
        "could -> 0.000855261849355\n",
        "would -> 0.00242655687491\n",
        "should -> 0.000586749408278\n",
        "must -> 0.000497245261253\n",
        "may -> 0.000656363744854\n",
        "might -> 0.000377906398552\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "religion\n",
        "----------------------\n",
        "can -> 0.00208127109825\n",
        "could -> 0.00149749993655\n",
        "would -> 0.00172593213026\n",
        "should -> 0.00114216096855\n",
        "must -> 0.00137059316226\n",
        "may -> 0.00197974567882\n",
        "might -> 0.000304576258281\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "reviews\n",
        "----------------------\n",
        "can -> 0.00110554245283\n",
        "could -> 0.000982704402516\n",
        "would -> 0.00115467767296\n",
        "should -> 0.000442216981132\n",
        "must -> 0.000466784591195\n",
        "may -> 0.00110554245283\n",
        "might -> 0.000638757861635\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "romance\n",
        "----------------------\n",
        "can -> 0.00105681071663\n",
        "could -> 0.00275627659878\n",
        "would -> 0.00348461911971\n",
        "should -> 0.000456999228814\n",
        "must -> 0.000642655165519\n",
        "may -> 0.000157093484905\n",
        "might -> 0.000728342520922\n",
        "\n",
        "science_fiction\n",
        "----------------------\n",
        "can -> 0.00110573600553\n",
        "could -> 0.00338631651693\n",
        "would -> 0.0054595715273\n",
        "should -> 0.000207325501037\n",
        "must -> 0.000552868002764\n",
        "may -> 0.000276434001382\n",
        "might -> 0.000829302004147\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Vamos a repetir la operaci\u00f3n de c\u00e1lculo de frecuencias relativas reasignando estos valores en el propio objeto `modals_cfd`, con el objetivo de utilizar el m\u00e9todo `tabulate` para poder impirmir la tabla con los valors relativos."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# lo primero, realizo una copia de mi distribuci\u00f3n de frecuencias \n",
      "import copy\n",
      "modals_cfd_rel = copy.copy(modals_cfd)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sustituyo los conteos de la tabla por sus frecuencias relativas (ojo, en tantos por 10.000)\n",
      "for categoria in brown.categories():\n",
      "    longitud = len(brown.words(categories=categoria))\n",
      "    for palabra in modals:\n",
      "        modals_cfd_rel[categoria][palabra] = (modals_cfd[categoria][palabra]/longitud)*10000"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# imprimo la tabla        \n",
      "modals_cfd_rel.tabulate(conditions=brown.categories(), samples=modals)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                 can could would should must  may might\n",
        "      adventure    6   21   27    2    3    0    8\n",
        " belles_lettres   14   12   22    5    9   11    6\n",
        "      editorial   19    9   29   14    8   12    6\n",
        "        fiction    5   24   41    5    8    1    6\n",
        "     government   16    5   17   15   14   21    1\n",
        "        hobbies   32    7    9    8   10   15    2\n",
        "          humor    7   13   25    3    4    3    3\n",
        "        learned   20    8   17    9   11   17    7\n",
        "           lore   15   12   16    6    8   14    4\n",
        "        mystery    7   24   32    5    5    2    9\n",
        "           news    9    8   24    5    4    6    3\n",
        "       religion   20   14   17   11   13   19    3\n",
        "        reviews   11    9   11    4    4   11    6\n",
        "        romance   10   27   34    4    6    1    7\n",
        "science_fiction   11   33   54    2    5    2    8\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Brown es tambi\u00e9n un corpus anotado con informaci\u00f3n morfol\u00f3gica\n",
      "\n",
      "El corpus de Brown no solo est\u00e1 categorizado, tambi\u00e9n est\u00e1 anotado con informaci\u00f3n morfol\u00f3gica. Para acceder a la versi\u00f3n anotada del corpus, podemos utilizar los m\u00e9todos: `brown.tagged_words`, `brown.tagged_sents` y `brown.tagged_\n",
      "paras`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scifi_tagged_words = brown.tagged_words(categories=\"science_fiction\")\n",
      "print scifi_tagged_words[:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('Now', 'RB'), ('that', 'CS'), ('he', 'PPS'), ('knew', 'VBD'), ('himself', 'PPL'), ('to', 'TO'), ('be', 'BE'), ('self', 'NN'), ('he', 'PPS'), ('was', 'BEDZ'), ('free', 'JJ'), ('to', 'TO'), ('grok', 'VB'), ('ever', 'QL'), ('closer', 'RBR'), ('to', 'IN'), ('his', 'PP$'), ('brothers', 'NNS'), (',', ','), ('merge', 'VB')]\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "F\u00edjate que cuando accedemos a la versi\u00f3n etiquetada del corpus, no obtenemos una simple lista de palabras sino una lista de tuplas, donde el primer elemento es la palabra en cuesti\u00f3n u el segundo es la etiqueta que indica la categor\u00eda gramatical de la palabra.\n",
      "\n",
      "Este conjunto etiquetas se ha convertido en casi un est\u00e1ndar para el ingl\u00e9s y se utilizan habitualmente para anotar cualquier recurso ling\u00fc\u00edstico en esa lengua.\n",
      "\n",
      "Vamos a crear una nueva frecuencia de distribuci\u00f3n condicional para calcular la frecuencia de aparici\u00f3n de las etiquetas, teniendo en cuenta la categor\u00eda."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "tags_cfd = ConditionalFreqDist(\n",
      "                                (category, item[1])\n",
      "                                for category in brown.categories()\n",
      "                                for item in brown.tagged_words(categories=category)\n",
      "                              )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Y ahora vamos a imprimir la tabla de frecuencias para cada categor\u00eda y para algunas de las etiquetas morfol\u00f3gicas: sustantivos en singular `NN`, verbos en presente `VB`, verbos en pasado simple `VBD`, participios pasados `VBN`, adjetivos `JJ`, preposiciones `IN`, y art\u00edculos `AT`. \n",
      "\n",
      "Recuerda: estas cifras no son directamente comparables entre categor\u00edas ya que \u00e9stas no est\u00e1n equilibradas. Hay categor\u00edas con m\u00e1s textos que otras."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tags_cfd.tabulate(conditions=brown.categories(), \n",
      "                  samples=\"NN VB VBD VBN JJ IN AT\".split())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                  NN   VB  VBD  VBN   JJ   IN   AT\n",
        "      adventure 8051 2170 3702 1276 2687 5908 5531\n",
        " belles_lettres 21800 4829 3501 4223 10414 19083 14898\n",
        "      editorial 7675 2129  700 1491 3593 6204 5311\n",
        "        fiction 7815 2173 3027 1497 2958 6012 5439\n",
        "     government 9877 1833  405 2190 4173 8596 5716\n",
        "        hobbies 12465 2966  617 2252 4883 8591 6946\n",
        "          humor 2567  656  699  478 1078 1926 1655\n",
        "        learned 29194 4342 1481 6044 12294 21757 16828\n",
        "           lore 14707 3083 2272 2822 6475 12074 9936\n",
        "        mystery 6461 2026 2645 1161 2109 4692 4321\n",
        "           news 13162 2440 2524 2269 4392 10616 8893\n",
        "       religion 4923 1275  511  931 2327 4266 3327\n",
        "        reviews 5066  872  504  875 2742 4040 3447\n",
        "        romance 7166 2404 3048 1359 3180 5616 4671\n",
        "science_fiction 1541  495  531  318  723 1176 1040\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Veamos otro ejemplo: creamos una lista de adjetivos (etiquetados como JJ) que aparezcan en la colecci\u00f3n de textos sobre hobbies, e imprimirmos los 50 primeros que encontramos"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# creo una lista vac\u00eda de adjetivos\n",
      "adjetivos = []\n",
      "\n",
      "# itero sobre las tuplas de la categor\u00edas hobbies\n",
      "for tupla in brown.tagged_words(categories=\"hobbies\"):\n",
      "    # compruebo que son adjetivos\n",
      "    if tupla[1] == \"JJ\":\n",
      "        # guardo la palabra en cuesti\u00f3n en mis lista de adjetivos\n",
      "        adjetivos.append(tupla[0])\n",
      "        \n",
      "# hay bastantes \n",
      "print len(adjetivos)  \n",
      "# as\u00ed que, solo imprimo solo los 50 primeros\n",
      "print adjetivos[:50]    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4883\n",
        "['old', 'childish', 'genuine', 'happy', 'unkind', 'happy', 'sunny', 'new', 'fast', 'fast', 'good', 'famous', 'prize-winning', 'tall', 'astounding', 'famous', 'fast', 'wonder-working', 'crazy', 'vigorous', 'solid', 'skinny', 'shapely', 'upper', 'muscular', 'symmetrical', 'real', 'now-famous', 'specific', 'famous', 'upper', 'collar-to-collar', 'wide', 'Reeves-type', 'upper', 'frontal', 'entire', 'chest-back-shoulder', 'alternate', 'alternate', 'complete', 'complete', 'five-minute', 'complete', 'similar', 'flat', 'downward', 'possible', 'upper', 'true']\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Otro ejempo: Para cada categor\u00eda del corpus de Brown, imprimimos solo aquellos adjetivos que tengan una longitud de al menos 15 caracteres y que no sean palabras compuestas escritas con guiones ortogr\u00e1ficos."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# en este caso, creo un diccionario vac\u00edo\n",
      "adjetivos = {}\n",
      "\n",
      "# itero sobre las categor\u00edas\n",
      "for categoria in brown.categories():\n",
      "    for elemento in brown.tagged_words():\n",
      "        if elemento[1] == \"JJ\":\n",
      "            if len(elemento[0]) >= 15:\n",
      "                if \"-\" not in elemento[0]:\n",
      "                    # cuando encuentro una palabra que cumple las tres condiciones, la almacenos en mi diccionario de adjetivos\n",
      "                    adjetivos[elemento[0]] = 1\n",
      "\n",
      "# por \u00faltimo, imprimo las claves de mi diccionario\n",
      "print adjetivos.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['unreconstructed', 'cathodoluminescent', 'communicational', 'polycrystalline', 'phenomenological', 'unconstitutional', 'inconsequential', 'impressionistic', 'autobiographical', 'cathodophoretic', 'nondiscriminatory', 'incontrovertible', 'undifferentiated', 'noncommissioned', 'gastrointestinal', 'distinguishable', 'interdenominational', 'internationalist', 'expressionistic', 'bibliographical', 'neuropsychiatric', 'extraterrestrial', 'semiquantitative', 'spectrophotometric', 'psychopharmacological', 'micrometeoritic', 'indistinguishable', 'interdepartmental', 'intergovernmental', 'intercontinental', 'individualistic', 'glottochronological', 'photoelectronic', 'undistinguished', 'incomprehensible', 'macropathological', 'Thermogravimetric', 'disproportionate', 'crystallographic', 'particularistic', 'traditionalistic', 'underprivileged', 'lexicostatistic', 'straightforward', 'uncommunicative', 'substitutionary', 'pharmacological', 'substerilization', 'anthropomorphic', 'interchangeable', 'multidimensional', 'nonmythological', 'Crystallographic', 'intradepartmental', 'psychotherapeutic', 'chromatographic', 'intraepithelial', 'indiscriminating', 'parasympathetic', 'intercollegiate', 'Physicochemical', 'unsophisticated', 'encephalographic', 'nonagricultural', 'trichloroacetic', 'anthropological']\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Recursos L\u00e9xicos: WordNet\n",
      "\n",
      "Wordnet es una red sem\u00e1ntica para el ingl\u00e9s. En esencia, es similar a un diccionario pero est\u00e1 organizado por *synsets* (conjunto de palabras sin\u00f3nimas) y no por lemas. \n",
      "\n",
      "Podemos acceder a WordNet a trav\u00e9s de NLTK:\n",
      " "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from nltk.corpus import wordnet as wn"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Para consultar los synsets en los que aparece una determinada palabra, podemos utilizar el m\u00e9todo `.synsets` como se muestra en el ejemplo. Como resultado obtenemos una lista con todos los synsets en los que aparece la palabra."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# buscamos los synsets en los que aparece la palabra sword\n",
      "print wn.synsets(\"sword\")\n",
      "\n",
      "# y buscamos car\n",
      "print wn.synsets(\"car\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[Synset('sword.n.01')]\n",
        "[Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'), Synset('cable_car.n.01')]\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "En este caso, la palabra *sword* solo aparece en un synset, lo que implica que solo tiene un sentido. Adem\u00e1s, sabemos que es un sustantivo, porque el nombre de synset est\u00e1 etiquetado como `n`.\n",
      "\n",
      "Por su parte, la palabra *car* es polis\u00e9mica y aparece en cinco sentidos, toso ellos sustantivos.\n",
      "\n",
      "Si guardo el synset en cuesti\u00f3n en una variable (f\u00edjate que me quedo con el primer elemento de la lista que me devuelve el m\u00e9todo `wn.synsets`), podemos acceder a distintos m\u00e9todos:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sword = wn.synsets(\"sword\")[0]\n",
      "print sword.lemma_names # imprime los lemas del synset => sin\u00f3nimos\n",
      "print sword.definition # imprime la definici\u00f3n del synset\n",
      "\n",
      "# hacemos lo mismo con car\n",
      "car = wn.synsets(\"car\")\n",
      "cable_car = car[-1]\n",
      "print cable_car.lemma_names, cable_car.definition\n",
      "print car[-1].lemma_names, car[-1].definition # esta l\u00ednea es equivalente a la anterior. \u00bfVes por qu\u00e9?\n",
      "\n",
      "# imprimo las oraciones de ejemplo\n",
      "print cable_car.examples"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['sword', 'blade', 'brand', 'steel']\n",
        "a cutting or thrusting weapon that has a long metal blade and a hilt with a hand guard\n",
        "['cable_car', 'car'] a conveyance for passengers or freight on a cable railway\n",
        "['cable_car', 'car'] a conveyance for passengers or freight on a cable railway\n",
        "['they took a cable car to the top of the mountain']\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Si escribes `sword.` y pulsas el tabulador podr\u00e1s visualizar todos los m\u00e9todos accesibles desde un objeto synset. Son muchos: si tienes inter\u00e9s en alguno que no se menciona en este resumen, preg\u00fantame o consulta el libro de NLTK.\n",
      "\n",
      "Entre las cosas que s\u00ed nos interesan est\u00e1 el poder acceder a relaciones como hiponimia, meronimia, etc. Por ejemplo, para acceder a todos los hip\u00f3nimos de *sword* con el sentido de *espada*, es decir, a todos los **tipos de** *espada* y a sus definiciones."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print sword.hyponyms()\n",
      "\n",
      "for element in sword.hyponyms():\n",
      "    print element.lemma_names\n",
      "    print element.definition"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[Synset('rapier.n.01'), Synset('cutlas.n.01'), Synset('broadsword.n.01'), Synset('fencing_sword.n.01'), Synset('backsword.n.02'), Synset('cavalry_sword.n.01'), Synset('falchion.n.01')]\n",
        "['rapier', 'tuck']\n",
        "a straight sword with a narrow blade and two edges\n",
        "['cutlas', 'cutlass']\n",
        "a short heavy curved sword with one edge; formerly used by sailors\n",
        "['broadsword']\n",
        "a sword with a broad blade and (usually) two cutting edges; used to cut rather than stab\n",
        "['fencing_sword']\n",
        "a sword used in the sport of fencing\n",
        "['backsword']\n",
        "a sword with only one cutting edge\n",
        "['cavalry_sword', 'saber', 'sabre']\n",
        "a stout sword with a curved blade and thick back\n",
        "['falchion']\n",
        "a short broad slightly convex medieval sword with a sharp point\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "En lugar de bajar hasta los elementos m\u00e1s espec\u00edficos, podemos navegar en la jerarqu\u00eda de sentidos hasta los synsets m\u00e1s generales. Por ejemplo, podemos acceder a los hiper\u00f3nimos inmediatos de un synset a trav\u00e9s del m\u00e9todo `.hypernyms()`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for element in sword.hypernyms():\n",
      "    print element.lemma_names\n",
      "    print element.definition"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['weapon', 'arm', 'weapon_system']\n",
        "any instrument or instrumentality used in fighting or hunting\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "F\u00edjate que con este m\u00e9todo s\u00f3lo subimos un nivel hacia el synset m\u00e1s general. En este caso, comprobamos que *sword* es un tipo de *weapon* o *arm*. Si, por el contrario, lo que nos interesa es acceder a todos los hiper\u00f3nimos de *sword*, navegando hasta el elemento ra\u00edz de la jerarqu\u00eda de WordNet (que siempre es *entity*), podemos utilizar el m\u00e9todo `.hypernym_paths()`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for path in sword.hypernym_paths():\n",
      "    for element in path:\n",
      "        print element.lemma_names\n",
      "        print element.definition"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['entity']\n",
        "that which is perceived or known or inferred to have its own distinct existence (living or nonliving)\n",
        "['physical_entity']\n",
        "an entity that has physical existence\n",
        "['object', 'physical_object']\n",
        "a tangible and visible entity; an entity that can cast a shadow\n",
        "['whole', 'unit']\n",
        "an assemblage of parts that is regarded as a single entity\n",
        "['artifact', 'artefact']\n",
        "a man-made object taken as a whole\n",
        "['instrumentality', 'instrumentation']\n",
        "an artifact (or system of artifacts) that is instrumental in accomplishing some end\n",
        "['device']\n",
        "an instrumentality invented for a particular purpose\n",
        "['instrument']\n",
        "a device that requires skill for proper use\n",
        "['weapon', 'arm', 'weapon_system']\n",
        "any instrument or instrumentality used in fighting or hunting\n",
        "['sword', 'blade', 'brand', 'steel']\n",
        "a cutting or thrusting weapon that has a long metal blade and a hilt with a hand guard\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "F\u00edjate que `.hypernym_paths()` me devuelve una lista de caminos posibles desde el synset en cuesti\u00f3n hasta el elemento *entity*. Por eso itero sobre los elementos `path` que me devuelve `.hypernym_paths()`. En el ejemplo de *sword*, da la casualidad de que solo hay un camino posible. Cada `path` es una lista de synsets, e itero sobre ellos. Por eso utilizo un bucle dentro de otro.\n",
      "\n",
      "Para acceder a los mer\u00f3nimos, es decir, a las partes o elementos constitutivos de *sword*, podemos utilizar el m\u00e9todo `.part_meronyms()`, como se muestra a continuaci\u00f3n."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for element in sword.part_meronyms():\n",
      "    print element.lemma_names\n",
      "    print element.definition"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['forte']\n",
        "the stronger part of a sword blade between the hilt and the foible\n",
        "['blade']\n",
        "the flat part of a tool or weapon that (usually) has a cutting edge\n",
        "['point', 'tip', 'peak']\n",
        "a V shape\n",
        "['foible']\n",
        "the weaker part of a sword's blade from the forte to the tip\n",
        "['hilt']\n",
        "the handle of a sword or dagger\n",
        "['haft', 'helve']\n",
        "the handle of a weapon or tool\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "De manera similar, podemos acceder a los hol\u00f3nimos de un synset, es decir, a los elementos de los que *espada* forma parte, a trav\u00e9s del m\u00e9todo `.part_holonyms()`. El synset que estamos utilizando no tiene definidos hol\u00f3nimos, as\u00ed que el ejemplo devuelve una lista vac\u00eda."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print sword.part_holonyms()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[]\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Busquemos ahora alg\u00fan ejemplo que tenga otros tipos de mer\u00f3nimos."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# en cu\u00e1ntos synsets aparece la palabra water?\n",
      "water = wn.synsets(\"water\")\n",
      "for synset in water:\n",
      "    print synset.lemma_names\n",
      "\n",
      "# me quedo con el primero\n",
      "agua = water[0]\n",
      "# que tiene unos cuantos mer\u00f3nimos de sustancia\n",
      "print agua.substance_meronyms()\n",
      "\n",
      "# \u00eddem para air\n",
      "air = wn.synsets(\"air\")\n",
      "for synset in air:\n",
      "    print synset.lemma_names, synset.definition\n",
      "\n",
      "aire = air[0]\n",
      "print aire.substance_meronyms()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['water', 'H2O']\n",
        "['body_of_water', 'water']\n",
        "['water']\n",
        "['water_system', 'water_supply', 'water']\n",
        "['urine', 'piss', 'pee', 'piddle', 'weewee', 'water']\n",
        "['water']\n",
        "['water', 'irrigate']\n",
        "['water']\n",
        "['water']\n",
        "['water']\n",
        "[Synset('oxygen.n.01'), Synset('hydrogen.n.01')]\n",
        "['air'] a mixture of gases (especially oxygen) required for breathing; the stuff that the wind consists of\n",
        "['air'] the region above the ground\n",
        "['air', 'aura', 'atmosphere'] a distinctive but intangible quality surrounding a person or thing\n",
        "['breeze', 'zephyr', 'gentle_wind', 'air'] a slight wind (usually refreshing)\n",
        "['atmosphere', 'air'] the mass of air surrounding the Earth\n",
        "['air'] once thought to be one of four elements composing the universe (Empedocles)\n",
        "['tune', 'melody', 'air', 'strain', 'melodic_line', 'line', 'melodic_phrase'] a succession of notes forming a distinctive sequence\n",
        "['air', 'airwave'] medium for radio and television broadcasting\n",
        "['air_travel', 'aviation', 'air'] travel via aircraft\n",
        "['air_out', 'air', 'aerate'] expose to fresh air\n",
        "['air'] be broadcast\n",
        "['air', 'send', 'broadcast', 'beam', 'transmit'] broadcast over the airwaves, as in radio or television\n",
        "['publicize', 'publicise', 'air', 'bare'] make public\n",
        "['air'] expose to warm or heated air, so as to dry\n",
        "['vent', 'ventilate', 'air_out', 'air'] expose to cool or cold air so as to cool or freshen\n",
        "[Synset('argon.n.01'), Synset('nitrogen.n.01'), Synset('oxygen.n.01'), Synset('neon.n.01'), Synset('krypton.n.01'), Synset('xenon.n.01')]\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hay varios m\u00e9todos para acceder a distintos tipos de mer\u00f3nimos y hol\u00f3nimos, aunque no siempre est\u00e1n definidas estas relaciones. Cuando no est\u00e1n definidas, los m\u00e9todos no dan error, simplemente devuelven listas vac\u00edas.\n",
      "\n",
      "Los nombres de estos m\u00e9todos tratan de ser autoexplicativos: por un lado, tenemos `.part_holonyms()`, `.member_holonyms()`, `.substance_holonyms()`, y por otro, `.part_meronyms()`, `.member_meronyms()`, `.substance_meronyms()`."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Peque\u00f1o ejercicio\n",
      "\n",
      "- Busca los sentidos en los que aparece la palabra *bike*.\n",
      "- Identifica el que hace referencia a *bicicleta*: veh\u00edculo de dos ruedas a pedales\n",
      "- Imprime los mer\u00f3nimos, las partes que conforman una bicicleta\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bike_synsets = wn.synsets(\"bike\")\n",
      "for s in bike_synsets:\n",
      "    print s.pos, \":\", s.definition\n",
      "    \n",
      "print \"mer\u00f3nimos de\", bike_synsets[1].definition \n",
      "for meronym in bike_synsets[1].part_meronyms():\n",
      "    print meronym.lemma_names\n",
      "\n",
      "print \"----------------------------\"\n",
      "    \n",
      "print \"hip\u00f3nimos de\", bike_synsets[1].definition\n",
      "for hipo in bike_synsets[1].hyponyms():\n",
      "    print hipo.lemma_names"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "n : a motor vehicle with two wheels and a strong frame\n",
        "n : a wheeled vehicle that has two wheels and is moved by foot pedals\n",
        "v : ride a bicycle\n",
        "mer\u00f3nimos de a wheeled vehicle that has two wheels and is moved by foot pedals\n",
        "['kickstand']\n",
        "['chain']\n",
        "['bicycle_wheel']\n",
        "['pedal', 'treadle', 'foot_pedal', 'foot_lever']\n",
        "['bicycle_seat', 'saddle']\n",
        "['mudguard', 'splash_guard', 'splash-guard']\n",
        "['handlebar']\n",
        "['coaster_brake']\n",
        "['sprocket', 'sprocket_wheel']\n",
        "----------------------------\n",
        "hip\u00f3nimos de a wheeled vehicle that has two wheels and is moved by foot pedals\n",
        "['velocipede']\n",
        "['push-bike']\n",
        "['bicycle-built-for-two', 'tandem_bicycle', 'tandem']\n",
        "['safety_bicycle', 'safety_bike']\n",
        "['ordinary', 'ordinary_bicycle']\n",
        "['mountain_bike', 'all-terrain_bike', 'off-roader']\n"
       ]
      }
     ],
     "prompt_number": 38
    }
   ],
   "metadata": {}
  }
 ]
}