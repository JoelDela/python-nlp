{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[cuaderno de td-idf](http://www.cs.duke.edu/courses/spring14/compsci290/assignments/lab02.html)\n",
    "[calculand tf-idf](http://aimotion.blogspot.com.es/2011/12/machine-learning-with-python-meeting-tf.html)\n",
    "\n",
    "\n",
    "[tf-idf en Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "\n",
    "[The Vector Space Model](http://nlp.stanford.edu/IR-book/html/htmledition/scoring-term-weighting-and-the-vector-space-model-1.html)\n",
    "\n",
    "[Vector Representations of Words](https://www.tensorflow.org/versions/master/tutorials/word2vec/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esquemas de pesado para representar documentos\n",
    "\n",
    "Los modelos de espacio vectorial (*[vector space models](https://en.wikipedia.org/wiki/Vector_space_model) (VSMs)*) permiten representar palabras o términos dentro de un espacio vectorial continuo, de manera que las palabras que son similares desde el punto semántico se situan en puntos cercanos dentro de ese espacio común.\n",
    "\n",
    "El uso de distintas aproximaciones de modelos de espacio vectorial tiene una larga tradición en PLN. Todas ellas comparten una misma hipótesis distribucional: las palabras o términos que comparten contextos tienen significados similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# corpus ficticio con tres documentos \n",
    "d1 = 'los angeles times'\n",
    "d2 = 'new york times'\n",
    "d3 = 'new york post'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf (*term frequency*)\n",
    "\n",
    "**tf** es el peso que indica la frecuencia de un término, es decir, el número de veces que una determinada palabra aparece en un documento. \n",
    "\n",
    "La aproximación más sencilla consiste consiste en asignar como peso para el término $t$ en el documento $d$ del corpus $D$ (denotado como $\\mbox{tf}_{t,d}$) el número de ocurrencias de $t$ en $d$. Es recomendable normalizar esta frecuencia, diviendo el número de ocurrencias entre el número total de palabras de un documento, para no penalizar los documentos breves: $\\mathrm{tf}(t,d) = \\frac{\\mathrm{f}(t, d)}{\\max\\{\\mathrm{f}(w, d):w \\in d\\}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## idf (*inverse document frequency*)\n",
    "\n",
    "Trabajar con las frecuencias absolutas de los términos conlleva un problema: todos los términos presentes en la colección se consideran igualmente relevantes a la hora de discriminar la relevancia de los documentos. Y resulta que esto no es verdad. \n",
    "\n",
    "Imaginemos un corpus $D$ en el que la frecuencia total de dos términos concretos, *este* y *fonema*, es similar en términos absolutos. La distribución de estos términos a lo largo de la coleccion es seguramente muy diferente. El primero aparece con una distribución uniforme a lo largo del corpus, su capacidad discriminativa es baja y debería penalizarse a la hora de asignar relevancia (como el resto de *stopwords*). El segundo, por el contrario, se concentra principalmente en documentos que hablan de fonología, su capacidad discriminativa es alta y debería ser premiado.\n",
    "\n",
    "Existen mecanismos correctores para incorporar estas penalizaciones y premios en nuestros pesos. Los más habituales pasan por recurrir a la frecuencia de documento $\\mbox{df}_t$, definida como el número de documentos de la colección $D$ que contienen el término $t$: $\\mbox{df}_t = {|\\{d \\in D: t \\in d\\}|}$.\n",
    "\n",
    "Más concretamente, se calcula la frecuencia inversa de documento, o **idf** (*inverse document frequency*), definida como: $\\mbox{idf}_t = \\log {|D|\\over \\mbox{df}_t}$, donde $|D|$ indica el número total de documentos de nuestra colección. De este modo, el **idf** de un término específico pero muy discriminativo será alto, mientras que el de un término muy frecuente a lo largo de la coleccion será bajo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.idf\n",
    "\n",
    "**td.idf** (*term frequency - inverse document frequency*) es una medida numérica que expresa la relevancia de una palabra de un documento con respecto a una colección de documentos. Es uno de los esquemas de pesado más comunes en las tareas relacionadas con la recuperación de información y la minería de texto.\n",
    "\n",
    "El objetivo de esta métrica es representar los documentos de texto como vectores, ignorando el orden concreto de las palabras pero manteniendo la información relativa a las frecuencias de aparición. \n",
    "\n",
    "El valor de tf-idf de una palabra:\n",
    "\n",
    "- es mayor cuanto más frecuente sea esta palabra dentro de un documento concreto, pero;\n",
    "- es mayor cuando menos común sea la palabra en otros documentos de la colección.\n",
    "\n",
    "Estas dos características premian a los términos que son muy frecuentes en determinados documentos concretos pero poco comunes en general: estos términos pueden considerarse buenos descriptores de un conjunto de documentos. Y a la vez, penalizan aquellos términos que aparecen con mucha frecuencia a lo largo de toda la colección, como las *stopwords*.\n",
    "\n",
    "\n",
    "### Cómo se calcula\n",
    "\n",
    "**tf.idf** se calcula como el producto de dos términos: $\\mathrm{tf.idf}(t, d, D) = \\mathrm{tf}(t, d) \\times \\mathrm{idf}(t, D)$\n",
    "\n",
    "\n",
    "- la frecuencia de un término (tf): el número de veces que una determinada palabra aparece en un documento. \n",
    "\n",
    "- la frecuencia inversa de documento (idf): el logaritmo del número total de documentos en el corpus dividido entre el número de documentos en los que el término aparece."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
